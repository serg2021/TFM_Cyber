\chapter{Resultados}
\label{ch:res}

En este capítulo pasaremos a explicar cada experimento realizado para cada modelo, las métricas que se usarán para medir los resultados y comentaremos qué modelo consigue mejores prestaciones en base a éstos.
%TODO_DONE: introduce lo que se va a hacer en este capítulo

\section{Métricas}

A continuación vamos a explicar en qué consisten las métricas en las que nos basaremos para cotejar los experimentos realizados.

\subsection{Error Rate}

Para saber con cuánta veracidad y exactitud se han clasificado las imágenes usaremos esta métrica denominada \textbf{``Error Rate''} (o \textbf{Tasa de errores}). Simplemente indica el número de imágenes bien clasificadas junto con el número de imágenes mal clasificadas; a continuación se calcula en base a estos datos qué porcentaje de errores han habido durante la clasificación.

Realmente es una métrica muy común usada en una gran cantidad de ramas científicas, de modo que no es de sorprender que la hayamos escogido para este proyecto.

%\begin{figure}[H]
%  \centering
%  \begin{subfigure}[b]{0.45\linewidth}
    %\includegraphics[width=\linewidth]{Figuras/Iou_1.eps}
%    \caption{Imagen Original}
%  \end{subfigure}
%    \begin{subfigure}[b]{0.45\linewidth}
    %\includegraphics[width=\linewidth]{Figuras/IoU_2.eps}
%    \caption{Predicción}
%  \end{subfigure}
%  \caption{Imágenes para visualizar el \ac{mIoU}}
%\end{figure}

\subsection{Average Precision Score}

Con esta métrica se pretende evaluar la precisión media de los resultados de las predicciones. Para ello, se realiza la llamada curva \ac{PR} (o curva de \textbf{Precisión-Sensibilidad}); con ella se puede ver si la clasificación de las imágenes se ha realizado correctamente, es decir, sin falsos positivos en la misma (\textbf{Precisión}), y si se han detectado bien las imágenes por el modelo dentro de todo el set de imágenes , es decir, si han habido falsos negativos durante el proceso y si dentro de los mismos (y de los positivos reales) se han detectado correctamente los positivos (\textbf{Sensibilidad}). %cita

Es un poco confuso de entender, pero simplemente indica la efectividad del sistema en términos de falsos positivos y falsos negativos. En la curva los dos ejes están muy relacionados entre sí: si se aumenta la precisión, disminuirá la sensibilidad (y al revés).

Sin embargo, esta  curva no es la métrica que aquí explicamos. La precisión media se calcula a partir de la misma y es, a grandes rasgos, una forma de representar todos los valores de la curva en uno solo.

Para decirlo de forma resumida, la \textbf{precisión media} toma la curva \ac{PR} y realiza la media ponderada de los valores de la ``Precisión'' adquiridas en cada umbral de decisión, es decir, en cada valor de decisión que define la precisión del sistema (si variamos el umbral, variará a su vez la precisión del sistema, y por lo tanto servirá para dictaminar si el modelo es preciso o no); como pesos para realizar la media ponderada se utilizará el incremento de la sensibilidad del umbral anterior al actual. A continuación mostramos la fórmula para comprender mejor, de forma visual, lo explicado en este punto: %cita

\begin{equation}\label{eq:apscore}
AP = \sum_{n}(R_n - R_{n-1})P_n
\end{equation}

donde $P_n$ es la \textbf{Precisión} y $R_n$ es la \textbf{Sensibilidad} en el umbral \textbf{n}. Cuanto más cercana a 1 esta métrica, mejor será el sistema.

Esta métrica realmente es muy buena para nuestros objetivos porque está pensada para usarse en un set de imágenes \textbf{no balanceado}, es decir, con más elementos del set con una etiqueta que con otra (recordemos que nuestro set se compone de 5000 imágenes, de las cuales 250 tienen un exploit en su interior).
%TODO_DONE: yo suelo añadir una sección explicando las métricas, pero no es necesario, si las explicas en cada experimento. Recuerda traer aquí la explicación de la mIOU para evaluar la segmentación semántica.
%TODO_DONE: organiza en secciones los distintos experimentos.

\subsection{Balanced Accuracy}

\textbf{Balanced Accuracy} es una métrica diseñada precisamente para problemas de clasificación con set de imágenes no balanceados, tanto para problemas de clasificación binarios (con dos etiquetas) como de multiclase (con más de dos etiquetas).

Esta métrica se define como la media de la \textbf{Sensibilidad} obtenida de cada clase (recordemos que la \textbf{Sensibilidad} sirve para medir cuántos positivos han sido detectados por el modelo correctamente). Se utiliza para evaluar el rendimiento de un modelo de clasificación. Cuanto más cercano a 1 su valor, mejor será el modelo. %cita

\subsection{ROC AUC Score}

Como ya vimos en el capítulo \ref{ch:sota}, la curva \ac{ROC} se crea a partir de los valores de la \ac{TPR} y la \ac{FPR} en distintos umbrales. Consecuentemente, la \ac{AUC} se puede calcular a partir de la \ac{ROC},y es el área bajo la curva formada por ésta. Cuanto mayor sea su valor, más completo y eficaz será el modelo, ya que así se ve cómo aumenta el valor de la \ac{TPR} y cómo decrece el valor de la \ac{FPR}.

Todo este proceso se realiza bajo el contexto de la predicción realizada por el modelo para clasificar las imágenes. Se puede usar tanto para clasificación binaria como para multiclase. %cita

\section{Modelos}

A continuación vamos a explicar en qué consisten los modelos de Deep Learning usados para la clasificación de imágenes: \textbf{ResNet-34}, \textbf{ResNet-18} y \textbf{AlexNet}.

\subsection{ResNet-34}

\textbf{ResNet-34} es una \ac{CNN} de 34 capas utilizada, en parte, como un modelo de clasificación de imágenes, siendo ésta una de las tecnologías más avanzadas en esta materia. Este modelo ha sido entrenado bajo el set de imágenes de \textbf{ImageNet} (con más de 100.000 imágenes de 200 clases diferentes en su haber). No obstante, y antes de seguir, conviene explicar qué es una \ac{CNN}. %cita %cita

Una \ac{CNN} es una clase de red neuronal formada por capas convolucionales, mayormente usada para el análisis y la clasificación de imágenes. La primera capa extrae información de una imagen, generará una serie de funciones que, finalmente, se acabarán activando para tratar los datos de entrada, y el resultado se pasará a la siguiente capa. Este procedimiento sigue para el resto de capas.

Hay un detalle que se debe señalar: cuanto más profunda sea una \ac{CNN} (esto es, cuantas más capas tenga), mayor precisión y mayor detalle habrá para analizar las imágenes. Este dato es muy importante, ya que, como veremos en la siguiente sección entre ResNet-34 y ResNet-18, indicará por qué con uno de estos modelos se ha conseguido una mayor precisión en la clasificación que con el otro. %cita

\subsection{ResNet-18}

\textbf{ResNet-18}, al igual que ResNet-34, es una \ac{CNN} que se compone de 18 capas convolucionales. Ha sido preentrenada, como ResNet-34, bajo el set de imágenes de ImageNet.

En esencia, es una escisión de ResNet-34 que hemos escogido para ver si, usando una \ac{CNN} con menos capas, varía mucho o no la efectividad en la clasificación para un set de imágenes tan pequeño (en comparación con ImageNet).

\subsection{AlexNet}

\textbf{AlexNet} es una \ac{CNN} de 8 capas (5 de ellas convolucionales). En su momento, AlexNet surgió como solución para poder entrenar un modelo de Deep Learning con un set de imágenes muy grande (como ImageNet). En otras palabras, es una arquitectura líder en detección de objetos y con un amplio potencial para poder aplicarse en más ámbitos de visión por ordenador.

Una característica muy interesante, en comparación con los otros modelos ya vistos, es su gran efectividad para la poca profundidad de capas que lo compone. Además, sabiendo sus precedentes en cuanto a tareas de análisis y clasificación de imágenes, no podíamos no elegirlo como un potencial candidato para cumplir con los objetivos de este trabajo.

En el siguiente punto, pasamos a mostrar los datos arrojados por los modelos para cada métrica.

\newpage

\section{Resultados finales}

A continuación, vamos a mostrar en la siguiente tabla los resultados generados por cada modelo para cada una de las métricas ya explicadas:

\begin{table}[H]
\centering
\resizebox{16cm}{!}{
\begin{tabular}{|c|c|c|c|}\cline{1-4}
\textbf{Modelos} & \textbf{Métricas} & \textbf{Epoch} & \textbf{Resultados}\\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{ResNet-34}}} & \multirow{2}{3cm}{Error Rate} & 0 & 0.049 \\ \cline{3-4}
& & 24 & 0.002 \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & 0.60 \\ \cline{3-4}
& & 24 & 0.98 \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & \textbf{0.92}\\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & 0.88 \\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{ResNet-18}}} & \multirow{2}{3cm}{Error Rate} & 0 & \textbf{0.036} \\ \cline{3-4}
& & 24 & \textbf{0} \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & 0.57 \\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & 0.80\\ \cline{3-4}
& & 24 & 0.99 \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & \textbf{0.89} \\ \cline{3-4}
& & 24 & 0.99 \\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{AlexNet}}} & \multirow{2}{3cm}{Error Rate} & 0 & 0.049 \\ \cline{3-4}
& & 24 & 0.014 \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & \textbf{0.32} \\ \cline{3-4}
& & 24 & 0.90 \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & 0.59 \\ \cline{3-4}
& & 24 & 0.88 \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & 0.77 \\ \cline{3-4}
& & 24 & 0.99 \\ \cline{1-4}
\end{tabular}}
\caption{Tabla de resultados} %TODO_DONE: añade más información , sobre qué experimento, incluso comenta el ganador.
\label{tab:Resul_ISA2}
\end{table}

Los valores que aparecen \textbf{resaltados} indican que son los mejores valores de cada iteración (\textbf{``Epoch''}.

Como se puede apreciar para este experimento, los mejores resultados son los que da el sistema \textbf{\textit{\ac{SVR}}} en todos los niveles de agrupación para entornos urbanos (con la excepción del nivel 4 que, en autovías, es sensiblemente mejor usando Swiftnet, y en entornos urbanos el \ac{MAE} de ambos modelos está casi a la par). El sistema \textbf{\textit{Boosting Trees}} tampoco se queda atrás y en todos sus niveles consigue lo mismo que el sistema \textit{\ac{SVR}}: Una mejora del \ac{MAE} de Swiftnet con respecto del de DeepLab en entornos urbanos y, en el último nivel, vuelve a tener dicha mejora en autovías. El resto de sistemas (\textbf{\textit{Lasso y Lineal}}) siguen siendo peores ejecutándose con los resultados de Swiftnet, y en concreto el sistema de regresión \textit{Lineal}, para el nivel de agrupación 4, es el que da unos resultados realmente catastróficos a pesar de haber sido corregidos como explicamos más adelante.%TODO_DONE: da más detalles, donde se entrena, dódne se prueba, se trata de repetir lo del diseño del experimento

Es en este punto en que el que se puede ver la influencia que tiene \ac{SPP} sobre los descriptores de imagen y, por lo tanto, sobre los resultados de la tabla.

Cuando generamos los descriptores de imagen estamos utilizando \ac{SPP} para que, según el nivel de agrupación que se use, se obtenga más o menos información en éstos. Recordemos que el nivel de agrupación indica el nivel de granularidad que tendrán los descriptores, y por lo tanto la cantidad de información que llevarán consigo.

Analizando la tabla \ref{tab:Resul_ISA2}, vemos cómo para mayores niveles de agrupación, la precisión de los sistemas de regresión va en aumento. Esto puede verse en los niveles 3 y 4 de todos los sistemas, exceptuando el \textit{Lineal} que de ahora en adelante excluiremos por la poca certeza de sus resultados en comparación con el resto. Entre estos niveles, la diferencia entre el \ac{MAE} de un modelo en un determinado escenario con el \ac{MAE} del mismo modelo en el mismo escenario (pero en un distinto nivel) es muy pequeña. Esto es porque cuando aumentamos al nivel 4, el grado de granularidad es superior y tendrá más precisión para estimar la velocidad en comparación con el anterior. Sin embargo, el nivel 3 también aporta una granularidad muy alta y, por ende, una estimación muy precisa; por lo tanto, si con el nivel 3 ya obtenemos resultados concisos, con el nivel 4 podemos refinarlos hasta obtener una estimación con mayor nivel de certeza.

Esto no siempre es así, porque puede pasar que en el nivel 4 se tomen en consideración datos que el nivel 3 había pasado por alto y como resultado nos dé un \ac{MAE} mayor en el último nivel. A lo que nos referimos con todo esto es a la posibilidad de que mientras los sistemas de regresión se ejecutan con datos de nivel 3, al no tener la misma profundidad que los de nivel 4, pueden obviar información relevante que con un nivel superior serían capaces de procesar. Esto se puede ver, por ejemplo, en la estimación del sistema \textit{Lasso} tanto para Swiftnet como para DeepLab. Sin embargo, este suceso no ocurre sólo en los niveles 3 y 4, para comprobarlo basta con revisar los niveles iniciales del sistema \textit{\ac{SVR}}.

Dicho todo esto, y comparando los valores de la tabla, podemos asegurar que los mejores sistemas con Swiftnet son \textbf{\textit{Boosting Trees}} y \textbf{\textit{\ac{SVR}}}. Para concretar más, vamos a puntualizar cómo el sistema \textit{\ac{SVR}} es mejor incluso, y es debido al reducido \ac{MAE} que tiene en todos sus niveles en comparación con el sistema \textit{Boosting Trees}.

En conclusión, podemos afirmar que el mejor sistema de regresión que se puede utilizar para el modelo Swiftnet es \textbf{\textit{\ac{SVR}}}.

%TODO: faltan muchas cosas.
%TODO_DONE: 1) Hay que aclarar que los anteriores resultados, para cada regresor, han sido obtenidos con unos parámetros para el spatial pyramids concretos. Debemos ponerlos.
%TODO_DONE: 2) Hay que enriquecer este capítulo con un análisis de la influencia del nivel de la pirámide para cada regresor, así se entenderá porqué hemos puesto los ganadores en la primera tabla. Básicamente una tabla donde comparemos todos con todos los niveles del 1 al 4.
%TODO_DONE: 3)falta una sección con los resultados de evaluación en cityscapes en términos de segmentación semántica, donde podamos ver que replicamos los resultados del paper original. Yo pondría esta subsección la primera.

%4)TODO_DONE: Resultados cualitativos: añadir imágenes donde se vea la predicción de la velocidad, y la velocidad anotada, hay que seleccionar las mejores y las peores, y comentarlo. Puedes hacerlo para el modelo ganador nuestro, y creas dos figuras, una con imágenes urbanas y otra con imágenes de autopista. Puedes poner las 4 mejores y las 4 peores. 


%5) Para el examen conviene que tengas una demo lista en la que podamos ver el sistema funcionando. Lo lanzas contra una carpeta de imágenes y va una a una generando la velocidad, que se guarda en una imagen donde se vea el dato superpuesto, como si fuera un video. Si tienes dudas comentamos. Eso te va a permitir generar un gif a modo de video que te va a quedar muy chulo. 


%TODO_DONE: 6) Hecho en falta un análsis tiempos de procesado del switnet, para terminar de vender lo de que es tiempo real, vaya. Debes añadirlo.
