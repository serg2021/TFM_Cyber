\chapter{Resultados}
\label{ch:res}

En este capítulo pasaremos a explicar cada experimento realizado para cada modelo, las métricas que se usarán para medir los resultados y comentaremos qué modelo consigue mejores prestaciones en base a éstos.
%TODO_DONE: introduce lo que se va a hacer en este capítulo

\section{Métricas}

A continuación vamos a explicar en qué consisten las métricas en las que nos basaremos para cotejar los experimentos realizados.

\subsection{Error Rate}

Para saber con cuánta veracidad y exactitud se han clasificado las imágenes usaremos esta métrica denominada \textbf{``Error Rate''} (o \textbf{Tasa de errores}). Simplemente indica el número de imágenes bien clasificadas junto con el número de imágenes mal clasificadas; a continuación se calcula en base a estos datos qué porcentaje de errores han habido durante la clasificación.

Realmente es una métrica muy común usada en una gran cantidad de ramas científicas, de modo que no es de sorprender que la hayamos escogido para este proyecto.

%\begin{figure}[H]
%  \centering
%  \begin{subfigure}[b]{0.45\linewidth}
    %\includegraphics[width=\linewidth]{Figuras/Iou_1.eps}
%    \caption{Imagen Original}
%  \end{subfigure}
%    \begin{subfigure}[b]{0.45\linewidth}
    %\includegraphics[width=\linewidth]{Figuras/IoU_2.eps}
%    \caption{Predicción}
%  \end{subfigure}
%  \caption{Imágenes para visualizar el \ac{mIoU}}
%\end{figure}

\subsection{Average Precision Score}

Con esta métrica se pretende evaluar la precisión media de los resultados de las predicciones. Para ello, se realiza la llamada curva \ac{PR} (o curva de \textbf{Precisión-Sensibilidad}); con ella se puede ver si la clasificación de las imágenes se ha realizado correctamente, es decir, sin falsos positivos en la misma (\textbf{Precisión}), y si se han detectado bien las imágenes por el modelo dentro de todo el set de imágenes , es decir, si han habido falsos negativos durante el proceso y si dentro de los mismos (y de los positivos reales) se han detectado correctamente los positivos (\textbf{Sensibilidad}). %cita

Es un poco confuso de entender, pero simplemente indica la efectividad del sistema en términos de falsos positivos y falsos negativos. En la curva los dos ejes están muy relacionados entre sí: si se aumenta la precisión, disminuirá la sensibilidad (y al revés).

Sin embargo, esta  curva no es la métrica que aquí explicamos. La precisión media se calcula a partir de la misma y es, a grandes rasgos, una forma de representar todos los valores de la curva en uno solo.

Para decirlo de forma resumida, la \textbf{precisión media} toma la curva \ac{PR} y realiza la media ponderada de los valores de la ``Precisión'' adquiridas en cada umbral de decisión, es decir, en cada valor de decisión que define la precisión del sistema (si variamos el umbral, variará a su vez la precisión del sistema, y por lo tanto servirá para dictaminar si el modelo es preciso o no); como pesos para realizar la media ponderada se utilizará el incremento de la sensibilidad del umbral anterior al actual. A continuación mostramos la fórmula para comprender mejor, de forma visual, lo explicado en este punto: %cita

\begin{equation}\label{eq:apscore}
AP = \sum_{n}(R_n - R_{n-1})P_n
\end{equation}

donde $P_n$ es la \textbf{Precisión} y $R_n$ es la \textbf{Sensibilidad} en el umbral \textbf{n}. Cuanto más cercana a 1 esta métrica, mejor será el sistema.

Esta métrica realmente es muy buena para nuestros objetivos porque está pensada para usarse en un set de imágenes \textbf{no balanceado}, es decir, con más elementos del set con una etiqueta que con otra (recordemos que nuestro set se compone de 5000 imágenes, de las cuales 250 tienen un exploit en su interior).
%TODO_DONE: yo suelo añadir una sección explicando las métricas, pero no es necesario, si las explicas en cada experimento. Recuerda traer aquí la explicación de la mIOU para evaluar la segmentación semántica.
%TODO_DONE: organiza en secciones los distintos experimentos.

\subsection{Balanced Accuracy}

\textbf{Balanced Accuracy} es una métrica diseñada precisamente para problemas de clasificación con set de imágenes no balanceados, tanto para problemas de clasificación binarios (con dos etiquetas) como de multiclase (con más de dos etiquetas).

Esta métrica se define como la media de la \textbf{Sensibilidad} obtenida de cada clase (recordemos que la \textbf{Sensibilidad} sirve para medir cuántos positivos han sido detectados por el modelo correctamente). Se utiliza para evaluar el rendimiento de un modelo de clasificación. Cuanto más cercano a 1 su valor, mejor será el modelo. %cita

\subsection{ROC AUC Score}

Como ya vimos en el capítulo \ref{ch:sota}, la curva \ac{ROC} se crea a partir de los valores de la \ac{TPR} y la \ac{FPR} en distintos umbrales. Consecuentemente, la \ac{AUC} se puede calcular a partir de la \ac{ROC},y es el área bajo la curva formada por ésta. Cuanto mayor sea su valor, más completo y eficaz será el modelo, ya que así se ve cómo aumenta el valor de la \ac{TPR} y cómo decrece el valor de la \ac{FPR}.

Todo este proceso se realiza bajo el contexto de la predicción realizada por el modelo para clasificar las imágenes. Se puede usar tanto para clasificación binaria como para multiclase. %cita

\section{Modelos}

A continuación vamos a explicar en qué consisten los modelos de Deep Learning usados para la clasificación de imágenes: \textbf{ResNet-34}, \textbf{ResNet-18} y \textbf{AlexNet}.

\subsection{ResNet-34}

\textbf{ResNet-34} es una \ac{CNN} de 34 capas utilizada, en parte, como un modelo de clasificación de imágenes, siendo ésta una de las tecnologías más avanzadas en esta materia. Este modelo ha sido entrenado bajo el set de imágenes de \textbf{ImageNet} (con más de 100.000 imágenes de 200 clases diferentes en su haber). No obstante, y antes de seguir, conviene explicar qué es una \ac{CNN}. %cita %cita

Una \ac{CNN} es una clase de red neuronal formada por capas convolucionales, mayormente usada para el análisis y la clasificación de imágenes. La primera capa extrae información de una imagen, generará una serie de funciones que, finalmente, se acabarán activando para tratar los datos de entrada, y el resultado se pasará a la siguiente capa. Este procedimiento sigue para el resto de capas.

Hay un detalle que se debe señalar: cuanto más profunda sea una \ac{CNN} (esto es, cuantas más capas tenga), mayor precisión y mayor detalle habrá para analizar las imágenes. Este dato es muy importante, ya que, como veremos en la siguiente sección entre ResNet-34 y ResNet-18, indicará por qué con uno de estos modelos se ha conseguido una mayor precisión en la clasificación que con el otro. %cita

\subsection{ResNet-18}

\textbf{ResNet-18}, al igual que ResNet-34, es una \ac{CNN} que se compone de 18 capas convolucionales. Ha sido preentrenada, como ResNet-34, bajo el set de imágenes de ImageNet.

En esencia, es una escisión de ResNet-34 que hemos escogido para ver si, usando una \ac{CNN} con menos capas, varía mucho o no la efectividad en la clasificación para un set de imágenes tan pequeño (en comparación con ImageNet).

\subsection{AlexNet}

\textbf{AlexNet} es una \ac{CNN} de 8 capas (5 de ellas convolucionales). En su momento, AlexNet surgió como solución para poder entrenar un modelo de Deep Learning con un set de imágenes muy grande (como ImageNet). En otras palabras, es una arquitectura líder en detección de objetos y con un amplio potencial para poder aplicarse en más ámbitos de visión por ordenador.

Una característica muy interesante, en comparación con los otros modelos ya vistos, es su gran efectividad para la poca profundidad de capas que lo compone. Además, sabiendo sus precedentes en cuanto a tareas de análisis y clasificación de imágenes, no podíamos no elegirlo como un potencial candidato para cumplir con los objetivos de este trabajo.

En el siguiente punto, pasamos a mostrar los datos arrojados por los modelos para cada métrica.

\newpage

\section{Resultados finales}

A continuación, vamos a mostrar en la siguiente tabla los resultados generados por cada modelo para cada una de las métricas ya explicadas:

\begin{table}[H]
\centering
\resizebox{16cm}{!}{
\begin{tabular}{|c|c|c|c|}\cline{1-4}
\textbf{Modelos} & \textbf{Métricas} & \textbf{Epoch} & \textbf{Resultados}\\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{ResNet-34}}} & \multirow{2}{3cm}{Error Rate} & 0 & 0.049 \\ \cline{3-4}
& & 24 & 0.002 \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & \textbf{0.60} \\ \cline{3-4}
& & 24 & 0.98 \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & \textbf{0.92}\\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & 0.88 \\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{ResNet-18}}} & \multirow{2}{3cm}{Error Rate} & 0 & \textbf{0.036} \\ \cline{3-4}
& & 24 & \textbf{0} \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & 0.57 \\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & 0.80\\ \cline{3-4}
& & 24 & 0.99 \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & \textbf{0.89} \\ \cline{3-4}
& & 24 & 0.99 \\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{AlexNet}}} & \multirow{2}{3cm}{Error Rate} & 0 & 0.049 \\ \cline{3-4}
& & 24 & 0.014 \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & 0.32 \\ \cline{3-4}
& & 24 & 0.90 \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & 0.59 \\ \cline{3-4}
& & 24 & 0.88 \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & 0.77 \\ \cline{3-4}
& & 24 & 0.99 \\ \cline{1-4}
\end{tabular}}
\caption{Tabla de resultados} %TODO_DONE: añade más información , sobre qué experimento, incluso comenta el ganador.
\label{tab:Resul_ISA2}
\end{table}

Los valores que aparecen \textbf{resaltados} indican que son los mejores valores de cada iteración (\textbf{``Epoch''}) entre todos los modelos para cada una de las métricas.

Conviene destacar algo muy importante: la primera iteración, independientemente de su valor, indica más o menos cómo se va a desempeñar el modelo para las siguientes; sin embargo, lo que realmente importa es ver si en la última iteración ha habido una gran mejoría o, al menos, si se ha conseguido el mejor resultado posible para esa métrica.

\subsection{Error Rate}

En términos de \textbf{``Error Rate''}, el mejor modelo es \textbf{ResNet-18}: tanto para la primera iteración como para la última se obtienen los mejores resultados para esta métrica (\textbf{0.036} y \textbf{0}). Con \textbf{ResNet-34} se obtienen resultados muy parejos con el anterior modelo, sobre todo para la última iteración (\textbf{0.049} y \textbf{0.002}), mientras que con \textbf{AlexNet} hemos conseguido los peores resultados (\textbf{0.049} y \textbf{0.014}).

Un apunte muy interesante es ver cómo con ResNet-34 se obtiene un valor en la última iteración muy parecido al de ResNet-18; recordemos que ResNet-18 es un modelo muy parecido a ResNet-34 pero con menos capas convolucionales, es decir, ResNet-34 es un modelo más ``completo''. De esta forma, al entrenar ambos modelos con nuestro set de imágenes se puede entender por qué con ResNet-34 no se llega a obtener el mejor valor de esta métrica: porque la \ac{CNN}, al tener una mayor profundidad, necesitaría de más iteraciones para llegar a entrenarse de manera óptima. Sin embargo, con ResNet-18, se entiende que al tener pocas capas no necesita de tantas iteraciones para obtener dicho valor... En resumen, para pocas iteraciones es mejor usar ResNet-18, mientras que para un número mayor de iteraciones ResNet-34 conseguiría resultados mucho mejores.

\subsection{Average Precision Score}

Para esta métrica, el mejor valor en la primera iteración se obtiene con \textbf{ResNet-34} (\textbf{0.60}), mientras que para la última iteración se obtiene con \textbf{ResNet-18} (\textbf{1}).

Con esta métrica medíamos en un solo valor de 0 a 1 la curva \ac{PR}, por lo que como se puede ver para ambos modelos, se obtienen cifras muy parecidas y realmente buenas que indican que la precisión del modelo (junto con la sensibilidad del mismo) son las mejores que cabrían esperar para el set de imágenes que hemos entrenado.

Por el contrario, con \textbf{AlexNet} no se consiguen datos tan brillantes, pero ello tampoco es indicativo de que sea un mal modelo. Simplemente, no consigue las mismas prestaciones que ambos ResNet.

\subsection{Balanced Accuracy}

Dentro de \textbf{Balanced Accuracy} el mejor modelo sin lugar a dudas es \textbf{ResNet-34}, obteniendo para la primera y última iteración estos valores: \textbf{0.92} y \textbf{1}.

Conviene señalar un punto muy importante para la primera iteración de esta métrica: el valor obtenido nos indica que este modelo puede ser un fuerte candidato a ser el mejor de todos para los objetivos de este trabajo, ya que, como dijimos antes, la primera iteración nos indica cómo se va a desempeñar el modelo para las siguientes iteraciones (prueba de ello es cómo en la última iteración se obtiene el mejor valor posible para esta métrica). Dicho de otra forma, \textbf{ResNet-34} es, para esta métrica, con mucha diferencia el mejor modelo posible.

Recordemos que esta métrica es muy importante para sets de imágenes no equilibrados, esto es, que hay más elementos de una clase que de otra. De este modo, al estar trabajando con un set de imágenes con tan pocos elementos con exploits en su interior, en comparación con el resto de imágenes, podemos declarar que esta métrica tomará un papel muy importante para decidir qué modelo es el mejor.

\subsection{ROC AUC Score}

Por último, pasamos a observar y comentar los resultados obtenidos para \textbf{ROC AUC Score}. Al igual que con el resto de métricas, los mejores valores oscilan entre los dos modelos más representativos del trabajo: \textbf{ResNet-34} y \textbf{ResNet-18}.

A pesar de que el mejor valor para la primera iteración lo consiguió el modelo \textbf{ResNet-18} con \textbf{0.89}, y de que el mejor valor para la última iteración lo obtenemos con el modelo \textbf{ResNet-34} con \textbf{1}; se podría decir que para esta métrica ambos modelos son los mejores, ya que se obtienen valores realmente muy cercanos y parecidos entre sí para cada una de las iteraciones vistas en la tabla.

A modo de conclusión, para \textbf{ROC AUC Score} lo mejor es utilizar los modelos de la familia ResNet. AlexNet, por el contrario, no ha resultado ser tan prometedor como cabría esperar.

\subsection{Conclusión}

Como ya hemos explicado en el punto anterior, la diferencia entre los modelos \textbf{ResNet-34} y \textbf{ResNet-18} reside en la profundidad de su red neuronal... aun así se puede apreciar cómo ambos modelos son de la misma familia aunque tengan sus diferencias, ya que los valores en dichas iteraciones son muy parecidos en la mayoría de las métricas.

Por otro lado, y como contraste, podemos ver cómo \textbf{AlexNet} ha acabado resultando en uno de los peores modelos, a pesar de tener buenos resultados. Sin embargo, no ha podido compararse con las tan buenas prestaciones servidas por los modelos de ResNet.

En definitiva, podemos aventurarnos a decidir cuál ha sido el mejor modelo de entre todos los experimentos realizados: en nuestra opinión, y viendo los resultados obtenidos, hemos decidido que el mejor modelo es \textbf{ResNet-34}.

Es una decisión un tanto controvertida, porque se ha observado cómo en varias métricas \textbf{ResNet-18} ha podido ser mejor, pero tiene un fundamento: \textbf{ResNet-34} es un modelo que a la larga, puede proporcionar mejores resultados que \textbf{ResNet-18}. Esto es, si entrenamos ambos modelos con un set de imágenes mayor, y con un número de iteraciones más grande, la precisión en la detección de malware será mejor en aquel modelo cuya profundidad, en cuanto a capas convolucionales se refiere, sea mayor; ya que analizará las imágenes con mayor detalle y las clasificará mejor.

En el siguiente capítulo, finalizaremos este trabajo señalando qué conclusiones e ideas finales hemos sacado en claro.

%TODO: faltan muchas cosas.
%TODO_DONE: 1) Hay que aclarar que los anteriores resultados, para cada regresor, han sido obtenidos con unos parámetros para el spatial pyramids concretos. Debemos ponerlos.
%TODO_DONE: 2) Hay que enriquecer este capítulo con un análisis de la influencia del nivel de la pirámide para cada regresor, así se entenderá porqué hemos puesto los ganadores en la primera tabla. Básicamente una tabla donde comparemos todos con todos los niveles del 1 al 4.
%TODO_DONE: 3)falta una sección con los resultados de evaluación en cityscapes en términos de segmentación semántica, donde podamos ver que replicamos los resultados del paper original. Yo pondría esta subsección la primera.

%4)TODO_DONE: Resultados cualitativos: añadir imágenes donde se vea la predicción de la velocidad, y la velocidad anotada, hay que seleccionar las mejores y las peores, y comentarlo. Puedes hacerlo para el modelo ganador nuestro, y creas dos figuras, una con imágenes urbanas y otra con imágenes de autopista. Puedes poner las 4 mejores y las 4 peores. 


%5) Para el examen conviene que tengas una demo lista en la que podamos ver el sistema funcionando. Lo lanzas contra una carpeta de imágenes y va una a una generando la velocidad, que se guarda en una imagen donde se vea el dato superpuesto, como si fuera un video. Si tienes dudas comentamos. Eso te va a permitir generar un gif a modo de video que te va a quedar muy chulo. 


%TODO_DONE: 6) Hecho en falta un análsis tiempos de procesado del switnet, para terminar de vender lo de que es tiempo real, vaya. Debes añadirlo.
