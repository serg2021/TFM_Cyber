\chapter{Resultados}
\label{ch:res}

En este capítulo pasaremos a explicar cada experimento realizado por cada modelo, las métricas que se usarán para medir los resultados y comentaremos qué modelo consigue mejores prestaciones en base a éstos.

\section{Métricas}

A continuación vamos a explicar en qué consisten las métricas en las que nos basaremos para cotejar los experimentos realizados.

\subsection{Error Rate}

Para saber con cuánta veracidad y exactitud se han clasificado las imágenes usaremos esta métrica denominada \textbf{``Error Rate''} (o \textbf{Tasa de errores}) (\cite{error-rate}). Simplemente indica el número de imágenes bien clasificadas junto con el número de imágenes mal clasificadas; a continuación se calcula en base a estos datos qué porcentaje de error ha habido durante la clasificación.

Realmente es una métrica muy común usada en una gran cantidad de ramas científicas, de modo que no es de sorprender que la hayamos escogido para este proyecto.

\subsection{Average Precision Score}

Con esta métrica se pretende evaluar la precisión media de los resultados de las predicciones. Para ello, se realiza la llamada curva \ac{PR} (o curva de \textbf{Precisión-Sensibilidad}) (\cite{curvas-pr}); con ella se puede ver si la clasificación de las imágenes se ha realizado correctamente, es decir, sin falsos positivos en la misma (\textbf{Precisión}), y si se han detectado bien las imágenes por el modelo dentro de todo el set de imágenes , es decir, si han habido falsos negativos durante el proceso y si dentro de los mismos (y de los positivos reales) se han detectado correctamente los positivos (\textbf{Sensibilidad}). %cita

De formaresumida, simplemente indica la efectividad del sistema en términos de falsos positivos y falsos negativos. En la curva los dos ejes están muy relacionados entre sí: si se aumenta la precisión, disminuirá la sensibilidad (y al revés).

Sin embargo, esta curva no es la métrica que aquí explicamos. La precisión media se calcula a partir de la misma y es, a grandes rasgos, una forma de representar todos los valores de la curva en uno solo.

Para decirlo de forma resumida, la \textbf{precisión media} toma la curva \ac{PR} y realiza la media ponderada de los valores de la ``Precisión'' adquiridas en cada umbral de decisión, es decir, en cada valor de decisión que define la precisión del sistema (si variamos el umbral, variará a su vez la precisión del sistema, y por lo tanto servirá para dictaminar si el modelo es preciso o no); como pesos para realizar la media ponderada se utilizará el incremento de la sensibilidad del umbral anterior al actual (\cite{apscore}). A continuación mostramos la fórmula para comprender mejor, de forma visual, lo explicado en este punto: %cita

\begin{equation}\label{eq:apscore}
AP = \sum_{n}(R_n - R_{n-1})P_n
\end{equation}

donde $P_n$ es la \textbf{Precisión} y $R_n$ es la \textbf{Sensibilidad} en el umbral \textbf{n}. Cuanto más cercana a 1 esta métrica, mejor será el sistema.

Esta métrica realmente es muy buena para nuestros objetivos porque está pensada para usarse en un set de imágenes \textbf{no balanceado}, es decir, con más elementos del set con una etiqueta que con otra (recordemos que nuestro set se compone de 5000 imágenes, de las cuales 250 tienen un exploit en su interior).

\subsection{Balanced Accuracy}

\textbf{Balanced Accuracy} es una métrica diseñada precisamente para problemas de clasificación con set de imágenes no balanceados, tanto para problemas de clasificación binarios (con dos etiquetas) como de multiclase (con más de dos etiquetas).

Esta métrica se define como la media de la \textbf{Sensibilidad} obtenida de cada clase (recordemos que la \textbf{Sensibilidad} sirve para medir cuántos positivos han sido detectados por el modelo correctamente). Se utiliza para evaluar el rendimiento de un modelo de clasificación. Cuanto más cercano a 1 su valor, mejor será el modelo (\cite{balanced-accuracy}, \cite{balanced-accuracy-2}). %cita

\subsection{ROC AUC Score}

Como ya vimos en el capítulo \ref{ch:sota}, la curva \ac{ROC} se crea a partir de los valores de la \ac{TPR} y la \ac{FPR} en distintos umbrales. Consecuentemente, la \ac{AUC} se puede calcular a partir de la \ac{ROC}, y es el área bajo la curva formada por ésta. Cuanto mayor sea su valor, más completo y eficaz será el modelo, ya que así se ve cómo aumenta el valor de la \ac{TPR} y cómo decrece el valor de la \ac{FPR}.

Todo este proceso se realiza bajo el contexto de la predicción realizada por el modelo para clasificar las imágenes. Se puede usar tanto para clasificación binaria como para multiclase (\cite{roc-auc-score}). %cita

\section{Modelos}

A continuación vamos a explicar en qué consisten los modelos de Deep Learning usados para la clasificación de imágenes: \textbf{ResNet-34}, \textbf{ResNet-18} y \textbf{AlexNet}.

\subsection{ResNet-34}

\textbf{ResNet-34} es una \ac{CNN} de 34 capas utilizada, en parte, como un modelo de clasificación de imágenes, siendo ésta una de las tecnologías más avanzadas en esta materia (\cite{resnet34}). Este modelo ha sido entrenado bajo el set de imágenes de \textbf{ImageNet} (con más de 100.000 imágenes de 200 clases diferentes en su haber) (\cite{imagenet}). No obstante, y antes de seguir, conviene explicar qué es una \ac{CNN}. %cita %cita

Una \ac{CNN} es una clase de red neuronal formada por capas convolucionales, mayormente usada para el análisis y la clasificación de imágenes. La primera capa extrae información de una imagen, generará una serie de funciones que, finalmente, se acabarán activando para tratar los datos de entrada y el resultado se pasará a la siguiente capa. Este procedimiento sigue para el resto de capas.

Hay un detalle que se debe señalar: cuanto más profunda sea una \ac{CNN} (esto es, cuantas más capas tenga), mayor precisión y mayor detalle habrá para analizar las imágenes (\cite{cnn}). Este dato es muy importante, ya que, como veremos en la siguiente sección entre ResNet-34 y ResNet-18, indicará por qué con uno de estos modelos se ha conseguido una mayor precisión en la clasificación que con el otro. %cita

\subsection{ResNet-18}

\textbf{ResNet-18}, al igual que ResNet-34, es una \ac{CNN} que se compone de 18 capas convolucionales y ha sido preentrenada bajo el set de imágenes de ImageNet (\cite{resnet18}).

En esencia, es una escisión de ResNet-34 que hemos escogido para ver si, usando una \ac{CNN} con menos capas, varía mucho o no la efectividad en la clasificación para un set de imágenes tan pequeño (en comparación con ImageNet).

\subsection{AlexNet}

\textbf{AlexNet} es una \ac{CNN} de 8 capas (5 de ellas convolucionales). En su momento, AlexNet surgió como solución para poder entrenar un modelo de Deep Learning con un set de imágenes muy grande (como ImageNet). En otras palabras, es una arquitectura líder en detección de objetos y con un amplio potencial para poder aplicarse en más ámbitos de visión por ordenador (\cite{alexnet}, \cite{alexnet-2}).

Una característica muy interesante, en comparación con los otros modelos ya vistos, es su gran efectividad para la poca profundidad de capas que lo compone. Además, sabiendo sus precedentes en cuanto a tareas de análisis y clasificación de imágenes, no podíamos no elegirlo como un potencial candidato para cumplir con los objetivos de este trabajo.

En el siguiente punto, pasamos a mostrar los datos arrojados por los modelos para cada métrica.

\newpage

\section{Resultados finales}

A continuación, vamos a mostrar en la siguiente tabla los resultados generados por cada modelo para cada una de las métricas ya explicadas:

\begin{table}[H]
\centering
\resizebox{16cm}{!}{
\begin{tabular}{|c|c|c|c|}\cline{1-4}
\textbf{Modelos} & \textbf{Métricas} & \textbf{Epoch} & \textbf{Resultados}\\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{ResNet-34}}} & \multirow{2}{3cm}{Error Rate} & 0 & 0.049 \\ \cline{3-4}
& & 24 & 0.002 \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & \textbf{0.60} \\ \cline{3-4}
& & 24 & 0.98 \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & \textbf{0.92}\\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & 0.88 \\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{ResNet-18}}} & \multirow{2}{3cm}{Error Rate} & 0 & \textbf{0.036} \\ \cline{3-4}
& & 24 & \textbf{0} \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & 0.57 \\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & 0.80\\ \cline{3-4}
& & 24 & 0.99 \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & \textbf{0.89} \\ \cline{3-4}
& & 24 & 0.99 \\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{AlexNet}}} & \multirow{2}{3cm}{Error Rate} & 0 & 0.049 \\ \cline{3-4}
& & 24 & 0.014 \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & 0.32 \\ \cline{3-4}
& & 24 & 0.90 \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & 0.59 \\ \cline{3-4}
& & 24 & 0.88 \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & 0.77 \\ \cline{3-4}
& & 24 & 0.99 \\ \cline{1-4}
\end{tabular}}
\caption{Tabla de resultados}
\label{tab:Resul_ISA2}
\end{table}

Los valores que aparecen \textbf{resaltados} indican que son los mejores valores de cada iteración (\textbf{``Epoch''}) de entre todos los modelos para cada una de las métricas.

Conviene destacar algo muy importante: la primera iteración, independientemente de su valor, indica más o menos cómo se va a desempeñar el modelo para las siguientes; sin embargo, lo que realmente importa es ver si en la última iteración ha habido una gran mejoría o, al menos, si se ha conseguido el mejor resultado posible para esa métrica.

\subsection{Error Rate}

En términos de \textbf{``Error Rate''}, el mejor modelo es \textbf{ResNet-18}: tanto para la primera iteración como para la última se obtienen los mejores resultados para esta métrica (\textbf{0.036} y \textbf{0}). Con \textbf{ResNet-34} se obtienen resultados muy parejos con el anterior modelo, sobre todo para la última iteración (\textbf{0.049} y \textbf{0.002}), mientras que con \textbf{AlexNet} hemos conseguido los peores resultados (\textbf{0.049} y \textbf{0.014}).

Un apunte muy interesante es ver cómo con ResNet-34 se obtiene un valor en la última iteración muy parecido al de ResNet-18; recordemos que ResNet-18 es un modelo muy parecido a ResNet-34 pero con menos capas convolucionales, es decir, ResNet-34 es un modelo más ``completo''. Sin embargo, ResNet-18 consigue un mejor resultado en la última iteración (aunque sea por muy poco). Esto no es un indicativo para elegir a ResNet-18 como el mejor modelo, porque aunque tenga mejor resultado, ResNet-34 se queda muy cerca y hay más métricas que valorar. No obstante, es interesante ver cómo dos modelos de la misma familia con diferente número de capas consiguen resultados muy parejos.

\subsection{Average Precision Score}

Para esta métrica, el mejor valor en la primera iteración se obtiene con \textbf{ResNet-34} (\textbf{0.60}), mientras que en la última iteración se obtiene con \textbf{ResNet-18} (\textbf{1}).

Con esta métrica medíamos en un solo valor de 0 a 1 la curva \ac{PR}, por lo que como se puede ver para ambos modelos, se obtienen cifras muy parecidas y realmente buenas que indican que la precisión del modelo (junto con la sensibilidad del mismo) son las mejores que cabrían esperar para el set de imágenes que hemos entrenado.

Por el contrario, con \textbf{AlexNet} no se consiguen datos tan brillantes, pero ello tampoco es indicativo de que sea un mal modelo. Simplemente, no consigue las mismas prestaciones que ambos ResNet.

\subsection{Balanced Accuracy}

Dentro de \textbf{Balanced Accuracy} el mejor modelo sin lugar a dudas es \textbf{ResNet-34}, obteniendo para la primera y última iteración estos valores: \textbf{0.92} y \textbf{1}.

Conviene señalar un punto muy importante para la primera iteración de esta métrica: el valor obtenido nos indica que este modelo puede ser un fuerte candidato a ser el mejor de todos para los objetivos de este trabajo, ya que, como dijimos antes, la primera iteración nos indica cómo se va a desempeñar el modelo para las siguientes iteraciones (prueba de ello es cómo en la última iteración se obtiene el mejor valor posible para esta métrica). Dicho de otra forma, \textbf{ResNet-34} es, para esta métrica y con mucha diferencia, el mejor modelo posible.

Recordemos que esta métrica es muy importante para sets de imágenes no equilibrados, esto es, que hay más elementos de una clase que de otra. De este modo, al estar trabajando con un set de imágenes con tan pocos elementos con exploits en su interior, en comparación con el resto de imágenes, podemos declarar que esta métrica tomará un papel muy importante para decidir qué modelo es el mejor.

\subsection{ROC AUC Score}

Por último, pasamos a observar y comentar los resultados obtenidos para \textbf{ROC AUC Score}. Al igual que con el resto de métricas, los mejores valores oscilan entre los dos modelos más representativos del trabajo: \textbf{ResNet-34} y \textbf{ResNet-18}.

A pesar de que el mejor valor para la primera iteración lo consiguió el modelo \textbf{ResNet-18} con \textbf{0.89}, y de que el mejor valor para la última iteración lo obtenemos con el modelo \textbf{ResNet-34} con \textbf{1}; se podría decir que para esta métrica ambos modelos son los mejores, ya que se obtienen valores realmente muy cercanos y parecidos entre sí para cada una de las iteraciones vistas en la tabla.

A modo de conclusión, para \textbf{ROC AUC Score} lo mejor es utilizar los modelos de la familia ResNet. AlexNet, por el contrario, no ha resultado ser tan prometedor como cabría esperar.

\subsection{Conclusión}

Como ya hemos explicado en el punto anterior, la diferencia entre los modelos \textbf{ResNet-34} y \textbf{ResNet-18} reside en la profundidad de su red neuronal... aun así se puede apreciar cómo ambos modelos son de la misma familia aunque tengan sus diferencias, ya que los valores en dichas iteraciones son muy parecidos en la mayoría de las métricas.

Por otro lado, y como contraste, podemos ver cómo \textbf{AlexNet} ha acabado resultando ser uno de los peores modelos. A pesar de tener buenos resultados, no ha podido compararse con las tan buenas prestaciones servidas por los modelos de ResNet.

En definitiva, podemos aventurarnos a decidir cuál ha sido el mejor modelo de entre todos los experimentos realizados: en nuestra opinión, y viendo los resultados obtenidos, hemos decidido que el mejor modelo es \textbf{ResNet-34}.

Es una decisión un tanto controvertida, porque se ha observado cómo en varias métricas \textbf{ResNet-18} ha podido ser mejor, pero tiene un fundamento: \textbf{ResNet-34} es un modelo que a la larga, puede proporcionar mejores resultados que \textbf{ResNet-18}. Esto es, si entrenamos ambos modelos con un set de imágenes mayor, y con un número de iteraciones más grande, la precisión en la detección de malware será mejor en aquel modelo cuya profundidad, en cuanto a capas convolucionales se refiere, sea mayor; ya que analizará las imágenes con mayor detalle y las clasificará mejor. Todo ello, sin olvidarnos de un motivo no menos importante: con la métrica \textbf{Balanced Accuracy} podemos ver el rendimiento de los modelos, y siendo ResNet-34 el que mejores valores ha obtenido podemos concluir en que este modelo, a la larga, será el apropiado para futuras aplicaciones.

En el siguiente capítulo, finalizaremos este trabajo señalando qué conclusiones e ideas finales hemos sacado en claro.
