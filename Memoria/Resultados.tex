\chapter{Resultados}
\label{ch:res}

En este capítulo pasaremos a explicar cada experimento realizado por cada modelo, las métricas que se usarán para medir los resultados y comentaremos qué modelo consigue mejores prestaciones en base a éstos.

\section{Métricas}

A continuación vamos a explicar en qué consisten las métricas en las que nos basaremos para cotejar los experimentos realizados.

\subsection{Error Rate}

Para saber con cuánta veracidad y exactitud se han clasificado las imágenes usaremos esta métrica denominada \textbf{``Error Rate''} (o \textbf{Tasa de errores}) (\cite{error-rate}). Simplemente indica el número de imágenes bien clasificadas junto con el número de imágenes mal clasificadas, usando para ello valores como los verdaderos y los falsos positivos; a continuación se calcula en base a estos datos qué porcentaje de error ha habido durante la clasificación.

A menudo se suele utilizar junto a la \textbf{precisión} y \textbf{sensibilidad}, dos métricas que a continuación veremos en el siguiente punto y que se sirven igualmente de las relaciones entre verdadero y falso positivo y verdadero y falso negativo (\cite{error-rate}).

\subsection{Average Precision Score}

Con esta métrica se pretende evaluar la precisión media de los resultados de las predicciones. Para ello, se realiza la llamada curva \ac{PR} (o curva de \textbf{Precisión-Sensibilidad}) (\cite{curvas-pr}); con ella se puede ver si la clasificación de las imágenes se ha realizado correctamente, es decir, sin falsos positivos en la misma (\textbf{Precisión}), y si se han detectado bien las imágenes por el modelo dentro de todo el set de imágenes , es decir, si han habido falsos negativos durante el proceso y si dentro de los mismos (y de los positivos reales) se han detectado correctamente los positivos (\textbf{Sensibilidad}). %cita

De forma resumida, simplemente indica la efectividad del sistema en términos de falsos positivos y falsos negativos. En la curva los dos ejes están muy relacionados entre sí: si se aumenta la precisión, disminuirá la sensibilidad (y al revés).

Sin embargo, esta curva no es la métrica que aquí explicamos. La precisión media se calcula a partir de la misma y es, a grandes rasgos, una forma de representar todos los valores de la curva en uno solo.

Para decirlo de forma resumida, la \textbf{precisión media} toma la curva \ac{PR} y realiza la media ponderada de los valores de la ``Precisión'' adquiridas en cada umbral de decisión, es decir, en cada valor de decisión que define la precisión del sistema (si variamos el umbral, variará a su vez la precisión del sistema, y por lo tanto servirá para dictaminar si el modelo es preciso o no); como pesos para realizar la media ponderada se utilizará el incremento de la sensibilidad del umbral anterior al actual (\cite{apscore}). A continuación mostramos la fórmula para comprender mejor, de forma visual, lo explicado en este punto: %cita

\begin{equation}\label{eq:apscore}
AP = \sum_{n}(R_n - R_{n-1})P_n
\end{equation}

donde $P_n$ es la \textbf{Precisión} y $R_n$ es la \textbf{Sensibilidad} en el umbral \textbf{n}. Cuanto más cercana a 1 esta métrica, mejor será el sistema.

Esta métrica realmente es muy buena para nuestros objetivos porque está pensada para usarse en un set de imágenes \textbf{no balanceado}, es decir, con más elementos del set con una etiqueta que con otra (recordemos que nuestro set se compone de 5000 imágenes, de las cuales 250 tienen un exploit en su interior). Por otro lado, suele ser utilizada comúnmente en trabajos de detección de objetos en imágenes, algo que no dista demasiado de nuestro trabajo; ya que igualmente hemos realizado una detección de la misma índole, con la diferencia de que los objetos detectados no se pueden ver a simple vista en la composición de la imagen (\cite{apscore-2}).

\subsection{Balanced Accuracy}

\textbf{Balanced Accuracy} es una métrica diseñada precisamente para problemas de clasificación con set de imágenes no balanceados, tanto para problemas de clasificación binarios (con dos etiquetas) como de multiclase (con más de dos etiquetas).

Esta métrica se define como la media de la \textbf{Sensibilidad} obtenida de cada clase (recordemos que la \textbf{Sensibilidad} sirve para medir cuántos positivos han sido detectados por el modelo correctamente). Cuanto más cercano a 1 su valor, mejor será el modelo (\cite{balanced-accuracy}, \cite{balanced-accuracy-2}). %cita

Como podemos comprobar, esta métrica toma como referencia los datos obtenidos en la métrica anterior (\textbf{Sensibilidad}), y sirve para evaluar el rendimiento de un modelo de clasificación en lo que a datos no balanceados se refiere: En nuestro caso, tenemos un set de imágenes con esas características, sin embargo, la precisión que obtenemos de los modelos no es correcta del todo debido a que la precisión se está haciendo en base a unos valores de verdaderos positivos y negativos muy altos (recordemos que el set de imágenes se compone de 5000 archivos, por lo que es lógico pensar que una gran mayoría serán detectados correctamente). No obstante, habrá un pequeño porcentaje de falsos positivos y negativos que no se corresponda con el porcentaje real de positivos y negativos. Al usar \textbf{Balanced Accuracy} se consiguen obtener los porcentajes reales (o, al menos, mejor estimados) de la precisión en la clasificación, ya que se le está dando a ambas clases el mismo peso; de modo que una, por más cantidad de objetos que tenga, no desvirtuará los datos de la precisión del modelo (\cite{balanced-accuracy-3}).

\subsection{ROC AUC Score}

Como ya vimos en el capítulo \ref{ch:sota}, la curva \ac{ROC} se crea a partir de los valores de la \ac{TPR} y la \ac{FPR} en distintos umbrales. Consecuentemente, la \ac{AUC} se puede calcular a partir de la \ac{ROC}, y es el área bajo la curva formada por ésta. Cuanto mayor sea su valor, más completo y eficaz será el modelo, ya que así se ve cómo aumenta el valor de la \ac{TPR} y cómo decrece el valor de la \ac{FPR}.

Todo este proceso se realiza bajo el contexto de la predicción realizada por el modelo para clasificar las imágenes. Se puede usar tanto para clasificación binaria como para multiclase (\cite{roc-auc-score}). %cita

Una vez más podemos ver cómo esta métrica se utiliza bajo los mismos parámetros que las otras: los verdaderos y falsos positivos. En esencia, podemos esclarecer de nuevo la precisión y el rendimiento del modelo viéndolo desde un enfoque más gráfico basado en dichos parámetros.

\section{Modelos}

A continuación vamos a explicar en qué consisten los modelos de Deep Learning usados para la clasificación de imágenes: \textbf{ResNet-34}, \textbf{ResNet-18} y \textbf{AlexNet}.

\subsection{ResNet-34}

\textbf{ResNet-34} es una \ac{CNN} de 34 capas utilizada, en parte, como un modelo de clasificación de imágenes, siendo ésta una de las tecnologías más avanzadas en esta materia (\cite{resnet34}). Este modelo ha sido entrenado bajo el set de imágenes de \textbf{ImageNet} (con más de 100.000 imágenes de 200 clases diferentes en su haber) (\cite{imagenet}). No obstante, y antes de seguir, conviene explicar qué es una \ac{CNN}. %cita %cita

Una \ac{CNN} es una clase de red neuronal formada por capas convolucionales, mayormente usada para el análisis y la clasificación de imágenes. La primera capa extrae información de una imagen, generará una serie de funciones que, finalmente, se acabarán activando para tratar los datos de entrada y el resultado se pasará a la siguiente capa. Este procedimiento sigue para el resto de capas.

Hay un detalle que se debe señalar: cuanto más profunda sea una \ac{CNN} (esto es, cuantas más capas tenga), mayor precisión y mayor detalle habrá para analizar las imágenes (\cite{cnn}). Este dato es muy importante, ya que, como veremos en la siguiente sección entre ResNet-34 y ResNet-18, indicará por qué con uno de estos modelos se ha conseguido una mayor precisión en la clasificación que con el otro. %cita

\subsection{ResNet-18}

\textbf{ResNet-18}, al igual que ResNet-34, es una \ac{CNN} que se compone de 18 capas convolucionales y ha sido preentrenada bajo el set de imágenes de ImageNet (\cite{resnet18}).

En esencia, es una escisión de ResNet-34 que hemos escogido para ver si, usando una \ac{CNN} con menos capas, varía mucho o no la efectividad en la clasificación para un set de imágenes tan pequeño (en comparación con ImageNet).

\subsection{AlexNet}

\textbf{AlexNet} es una \ac{CNN} de 8 capas (5 de ellas convolucionales). En su momento, AlexNet surgió como solución para poder entrenar un modelo de Deep Learning con un set de imágenes muy grande (como ImageNet). En otras palabras, es una arquitectura líder en detección de objetos y con un amplio potencial para poder aplicarse en más ámbitos de visión por ordenador (\cite{alexnet}, \cite{alexnet-2}).

Una característica muy interesante, en comparación con los otros modelos ya vistos, es su gran efectividad para la poca profundidad de capas que lo compone. Además, sabiendo sus precedentes en cuanto a tareas de análisis y clasificación de imágenes, no podíamos no elegirlo como un potencial candidato para cumplir con los objetivos de este trabajo.

En el siguiente punto, pasamos a mostrar los datos arrojados por los modelos para cada métrica.

\newpage

\section{Resultados finales}

A continuación, vamos a mostrar en la siguiente tabla los resultados generados por cada modelo para cada una de las métricas ya explicadas:

\begin{table}[H]
\centering
\resizebox{16cm}{!}{
\begin{tabular}{|c|c|c|c|}\cline{1-4}
\textbf{Modelos} & \textbf{Métricas} & \textbf{Epoch} & \textbf{Resultados}\\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{ResNet-34}}} & \multirow{2}{3cm}{Error Rate} & 0 & 0.049 \\ \cline{3-4}
& & 24 & 0.002 \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & \textbf{0.60} \\ \cline{3-4}
& & 24 & 0.98 \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & \textbf{0.92}\\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & 0.88 \\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{ResNet-18}}} & \multirow{2}{3cm}{Error Rate} & 0 & \textbf{0.036} \\ \cline{3-4}
& & 24 & \textbf{0} \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & 0.57 \\ \cline{3-4}
& & 24 & \textbf{1} \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & 0.80\\ \cline{3-4}
& & 24 & 0.99 \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & \textbf{0.89} \\ \cline{3-4}
& & 24 & 0.99 \\ \cline{1-4}
\multirow{4}{3cm}{\textbf{\textit{AlexNet}}} & \multirow{2}{3cm}{Error Rate} & 0 & 0.049 \\ \cline{3-4}
& & 24 & 0.014 \\ \cline{2-4}
& \multirow{2}{3cm}{Average Precision Score} & 0 & 0.32 \\ \cline{3-4}
& & 24 & 0.90 \\ \cline{2-4}
& \multirow{2}{3cm}{Balanced Accuracy} & 0 & 0.59 \\ \cline{3-4}
& & 24 & 0.88 \\ \cline{2-4}
& \multirow{2}{3cm}{ROC AUC Score} & 0 & 0.77 \\ \cline{3-4}
& & 24 & 0.99 \\ \cline{1-4}
\end{tabular}}
\caption{Tabla de resultados}
\label{tab:Resul_ISA2}
\end{table}

Los valores que aparecen \textbf{resaltados} indican que son los mejores valores de cada iteración (\textbf{``Epoch''}) de entre todos los modelos para cada una de las métricas.

Conviene destacar algo muy importante: la primera iteración, independientemente de su valor, indica más o menos cómo se va a desempeñar el modelo para las siguientes; sin embargo, lo que realmente importa es ver si en la última iteración ha habido una gran mejoría o, al menos, si se ha conseguido el mejor resultado posible para esa métrica.

\subsection{Error Rate}

En términos de \textbf{``Error Rate''}, el mejor modelo es \textbf{ResNet-18}: tanto para la primera iteración como para la última se obtienen los mejores resultados para esta métrica (\textbf{0.036} y \textbf{0}). Con \textbf{ResNet-34} se obtienen resultados muy parejos con el anterior modelo, sobre todo para la última iteración (\textbf{0.049} y \textbf{0.002}), mientras que con \textbf{AlexNet} hemos conseguido los peores resultados (\textbf{0.049} y \textbf{0.014}).

Un apunte muy interesante es ver cómo con ResNet-34 se obtiene un valor en la última iteración muy parecido al de ResNet-18; recordemos que ResNet-18 es un modelo muy parecido a ResNet-34 pero con menos capas convolucionales, es decir, ResNet-34 es un modelo más ``completo''. Sin embargo, ResNet-18 consigue un mejor resultado en la última iteración (aunque sea por muy poco). Esto no es un indicativo para elegir a ResNet-18 como el mejor modelo, porque aunque tenga mejor resultado, ResNet-34 se queda muy cerca y hay más métricas que valorar. No obstante, es interesante ver cómo dos modelos de la misma familia con diferente número de capas consiguen resultados muy parejos.

Con esta métrica, en esencia, hemos podido ver la poca tasa de error que han tenido todos los modelos, lo que nos hace pensar que la cantidad de verdaderos positivos es abrumadora con respecto a los falsos positivos. De hecho, tal y como veremos a continuación en la siguiente métrica, la precisión y la sensibilidad tendrán valores muy parejos, lo cual es un claro indicativo de que los modelos están efectuando bien la clasificación.

\subsection{Average Precision Score}

Para esta métrica, el mejor valor en la primera iteración se obtiene con \textbf{ResNet-34} (\textbf{0.60}), mientras que en la última iteración se obtiene con \textbf{ResNet-18} (\textbf{1}).

Con esta métrica medíamos en un solo valor de 0 a 1 la curva \ac{PR}, por lo que como se puede ver para ambos modelos, se obtienen cifras muy parecidas y realmente buenas que indican que la precisión del modelo (junto con la sensibilidad del mismo) son las mejores que cabrían esperar para el set de imágenes que hemos entrenado.

Por el contrario, con \textbf{AlexNet} no se consiguen datos tan brillantes, pero ello tampoco es indicativo de que sea un mal modelo. Simplemente, no consigue las mismas prestaciones que ambos ResNet.

Una vez más hemos podido comprobar que la clasificación de los modelos, en términos de verdaderos y falsos positivos, sigue siendo correcta. No obstante, en la siguiente métrica veremos cómo obtener una mayor fiabilidad en los datos por tratarse de un set de imágenes no balanceado, de forma que la precisión obtenida será más fiel a la realidad.

\subsection{Balanced Accuracy}

Dentro de \textbf{Balanced Accuracy} el mejor modelo sin lugar a dudas es \textbf{ResNet-34}, obteniendo para la primera y última iteración estos valores: \textbf{0.92} y \textbf{1}.

Conviene señalar un punto muy importante para la primera iteración de esta métrica: el valor obtenido nos indica que este modelo puede ser un fuerte candidato a ser el mejor de todos para los objetivos de este trabajo, ya que, como dijimos antes, la primera iteración nos indica cómo se va a desempeñar el modelo para las siguientes iteraciones (prueba de ello es cómo en la última iteración se obtiene el mejor valor posible para esta métrica). Dicho de otra forma, \textbf{ResNet-34} es, para esta métrica y con mucha diferencia, el mejor modelo posible.

Recordemos que esta métrica es muy importante para sets de imágenes no equilibrados, esto es, que hay más elementos de una clase que de otra. De este modo, al estar trabajando con un set de imágenes con tan pocos elementos con exploits en su interior, en comparación con el resto de imágenes, podemos declarar que esta métrica tomará un papel muy importante para decidir qué modelo es el mejor.

Esta métrica indica que la precisión en la clasificación se realiza dándole el mismo peso a ambas clases a pesar de estar claramente descompensadas, de modo que se obtienen unos valores adecuados a la realidad del set de imágenes bajo el que se mueve el modelo. Por otro lado, podemos ver que la sensibilidad toma un papel muy importante en el cálculo de esta métrica, por lo que se puede decir que tanto esta como la anterior están directamente relacionadas: prueba de ello es el comportamiento final de los modelos en la última iteración, donde se obtienen los mejores resultados para ambas métricas (todo ello sin contar con \textbf{``Error Rate''}, aunque igualmente es una métrica que sigue estando muy presente entre éstas).

\subsection{ROC AUC Score}

Por último, pasamos a observar y comentar los resultados obtenidos para \textbf{ROC AUC Score}. Al igual que con el resto de métricas, los mejores valores oscilan entre los dos modelos más representativos del trabajo: \textbf{ResNet-34} y \textbf{ResNet-18}.

A pesar de que el mejor valor para la primera iteración lo consiguió el modelo \textbf{ResNet-18} con \textbf{0.89}, y de que el mejor valor para la última iteración lo obtenemos con el modelo \textbf{ResNet-34} con \textbf{1}; se podría decir que para esta métrica ambos modelos son los mejores, ya que se obtienen valores realmente muy cercanos y parecidos entre sí para cada una de las iteraciones vistas en la tabla.

A modo de conclusión, para \textbf{ROC AUC Score} lo mejor es utilizar los modelos de la familia ResNet. AlexNet, por el contrario, no ha resultado ser tan prometedor como cabría esperar.

Al igual que con el resto de métricas, podemos comprobar que esta igualmente complementa al resto, ya que nos permite utilizar los verdaderos y falsos positivos de forma que podemos ver con qué exactitud se clasifican correctamente las imágenes. Con esta métrica se consigue para ResNet-34 un valor de 1, lo cual indica que para este modelo no ha habido ningún falso positivo en este experimento, algo que, en términos de eficacia, nos interesa que sea así.

\subsection{Conclusión}

Como ya hemos explicado en el punto anterior, la diferencia entre los modelos \textbf{ResNet-34} y \textbf{ResNet-18} reside en la profundidad de su red neuronal... aun así se puede apreciar cómo ambos modelos son de la misma familia aunque tengan sus diferencias, ya que los valores en dichas iteraciones son muy parecidos en la mayoría de las métricas.

Por otro lado, y como contraste, podemos ver cómo \textbf{AlexNet} ha acabado resultando ser un modelo no tan bueno. A pesar de tener buenos resultados, no ha podido compararse con las tan buenas prestaciones servidas por los modelos de ResNet.

Hay que tener en cuenta que las diferencias entre los modelos han sido muy pequeñas, por lo tanto se puede deducir que la arquitectura de la red de los modelos ha tenido poco que ver para destacar uno de ellos por encima del resto. Un factor muy relevante para obtener estos resultados ha podido ser la aleatoriedad en los entrenamientos, la cual ha sido suficiente como para presentar unos valores finales muy parejos entre sí.

Otro factor de relevancia es el rendimiento de cada modelo en una iteración (\textbf{``Epoch''} o inferencia). A continuación podemos ver el tiempo que ha tardado cada modelo en calcular el ``Error Rate'' para una inferencia:

\begin{figure}[H]
  \centering
  \begin{subfigure}[H]{0.45\linewidth}
  	\includegraphics[width=\linewidth]{Figuras/Resultados/Error\_Rate\_Resnet34.png}
  	\caption{Tiempo por inferencia con ResNet-34}
  \end{subfigure}
  \begin{subfigure}[H]{0.45\linewidth}
  	\includegraphics[width=\linewidth]{Figuras/Resultados/Error\_Rate\_Resnet18.png}
  	\caption{Tiempo por inferencia con ResNet-18}
  \end{subfigure}
  \begin{subfigure}[H]{0.45\linewidth}
  	\includegraphics[width=\linewidth]{Figuras/Resultados/Error\_Rate\_Alexnet.png}
  	\caption{Tiempo por inferencia con AlexNet}
  \end{subfigure}
  \caption{Rendimiento por inferencia}
\end{figure}

Podemos ver cómo el que mejor rendimiento presenta es AlexNet con casi 1 minuto por inferencia, seguido de ResNet-18 con un valor cercano al minuto y medio, y de ResNet-34 con más de 2 minutos y medio. En términos de rendimiento, deberíamos elegir ResNet-18 como un firme candidato para poder utilizarlo en páginas web o antivirus asegurando buenos resultados con un tiempo de procesado relativamente bajo. Igualmente no podemos olvidarnos de ResNet-34, puesto que se han obtenido valores similares y, en varias ocasiones mejores, pero el tiempo de procesado y el coste computacional derivado son demasiado altos para aplicaciones que necesitan una cierta rapidez en su proceso de detección de malware.

Finalmente, concluimos en decidir como mejor modelo \textbf{ResNet-34}.

Es una decisión un tanto controvertida, porque se ha observado cómo en varias métricas \textbf{ResNet-18} ha podido ser mejor, pero tiene un fundamento: \textbf{ResNet-34} es un modelo que a la larga, puede proporcionar mejores resultados que \textbf{ResNet-18}. Esto es, si entrenamos ambos modelos con un set de imágenes mayor, y con un número de iteraciones más grande, la precisión en la detección de malware será mejor en aquel modelo cuya profundidad, en cuanto a capas convolucionales se refiere, sea mayor; ya que analizará las imágenes con mayor detalle y las clasificará mejor. Todo ello, sin olvidarnos de un motivo no menos importante: con la métrica \textbf{Balanced Accuracy} podemos ver el rendimiento de los modelos, y siendo ResNet-34 el que mejores valores ha obtenido podemos concluir en que este modelo, a la larga, será el apropiado para futuras aplicaciones.

Por otro lado, y volviendo al tema del rendimiento, deberíamos considerar \textbf{ResNet-18} como un buen modelo a utilizar. Sin embargo, el baremo para tomar la decisión del mejor modelo se centra en las métricas utilizadas mayormente, junto con la arquitectura de la red que los conforman, es por ello que nos seguimos manteniendo en la misma postura con ResNet-34. Aun así, tomaremos en cuenta ResNet-18 para el próximo capítulo.

En el siguiente capítulo, finalizaremos este trabajo señalando qué conclusiones e ideas finales hemos sacado en claro, junto con una serie de trabajos futuros y aplicaciones a usar para estos modelos.
