\chapter{Implementación de la solución}
\label{ch:stego}

A continuación, en este capítulo, vamos a ver cómo fueron los primeros usos de Stegosploit junto con los problemas derivados de éstos. Más adelante pasaremos a ver qué solución propusimos para utilizar la herramienta y cómo insertamos el código malicioso en las imágenes. Por último, explicaremos cómo utilizamos FastAI con estas imágenes y qué modelos de Deep Learning acabamos usando. %cita %cita

\section{Primeros intentos de uso de Stegosploit}
\label{sec:stego_1}

En primera instancia, leímos el paper correspondiente a la herramienta original de \textbf{Stegosploit}, y tratamos de extraer la herramienta según las indicaciones provistas por el creador de la misma.

Sin embargo, y tras seguir sus pasos, obtuvimos un error (del que hablaremos más adelante) por el que no pudimos conseguir la herramienta y que, como veremos en otro contexto, nos obligó a tener que crear un \textbf{servidor HTTP} con algunas especificaciones sobre el que ejecutar la herramienta (una vez extraída de otra manera a la que proponía el creador). Durante el desarrollo del capítulo veremos cuál fue ese error, en qué consistía, y cómo se constituyó el servidor que creamos para solucionarlo.

Para obtener la herramienta, investigamos y encontramos un repositorio de GitHub llamado \textbf{PyStegosploit} que contenía todos los útiles de la misma. De este modo, descargamos el repositorio y lo colocamos en una ubicación fácilmente accesible de nuestro sistema de archivos.

Una vez obtenida la herramienta, ejecutamos la utilidad que propicia el encriptado de información en imágenes. Para ello nos situamos en la siguiente carpeta de la herramienta y abrimos el HTML propio de esta utilidad:

\begin{center}
\textit{PyStegosploit\textbackslash{project-stegosploit}\textbackslash{encoding}\textbackslash{iterative\_encoding.html}}
\end{center}

Si ahora ejecutamos la herramienta, nos dará el error del que hablábamos antes, y que ahora procedemos a explicar:

\subsection{Error \textit{getImageData()}}

Antes de explicar el error, vamos a poner en contexto al lector para que sepa en qué situación aparece y cómo solucionarlo.

%IMAGEN

Si ejecutamos la herramienta con una imagen del set de imágenes de COCO, y le introducimos un exploit, no funcionará. Saldrá el error que mencionamos debido a la siguiente función de Javascript: \textbf{getImageData()}.

Esta función permite obtener los datos de los píxeles de una imagen y almacenarlos en un elemento \textbf{Canvas} del HTML. En principio no debería de haber ningún problema con ello, sin embargo sucede una cosa muy importante: el origen de la imagen no es \textbf{`'fiable"}, y la política \ac{CORS} de la solicitud HTTP impide la ejecución de dicha función. %cita
%\begin{figure}[H]
%  \centering
%  \begin{subfigure}[b]{0.45\linewidth}
%    \includegraphics[width=\linewidth]{Figuras/Ejemplo_Highway.eps}
%    \caption{Highway}
%  \end{subfigure}
%    \begin{subfigure}[b]{0.45\linewidth}
%    \includegraphics[width=\linewidth]{Figuras/Ejemplo_Urban.eps}
%    \caption{Urban}
%  \end{subfigure}
%  \caption{Imágenes de $ISA^{2}$}
%  \label{fig:HyU}
%\end{figure}

%TODO_DONE: añade detalles de las secuencias, número de imágenes, figura con las mismas, etc.

%TODO_DONE: Empieza a dar más detalles técnicos de todas las partes del sistema. Hay que explicar todo.

%TODO_DONE: añade imagen con segmentación semántica y laimagen sin segmentar, y adaptas el texto de este párrafo que de dejo comentado.
%TODO_DONE: falta una breve descripción del modelo. Un párrafo es suficiente.
 

%TODO_DONE: Falta convertir estos item en párrafos como yo he hecho con el primero. Hay que explicar cada paso en detalle: los histogramas, las pirámides (qué son?, niveles?, cómo se computan?), los modelos de regresión que se probaron, etc.

\subsection{Spatial Pyramid Pooling (SPP)}

%\ac{SPP} (también conocido como ``multi-level pooling'') es una técnica usada para que cualquier imagen, en cualquier escala, pueda ser procesada correctamente por una \ac{CNN}. Consiste en la agrupación de distintas imágenes de entrada procesadas, con diferente escalado, para obtener una salida uniforme. Existen \ac{CNN}s que requieren de una escala fija en una imagen de entrada para conseguir un procesamiento óptimo, por lo que se limita la precisión de la predicción. Aplicando \ac{SPP} al final de una \ac{CNN} esta limitación desaparece.

%TODO_DONE: el párrafo anterior estaba mal, cuidado porque demuestra que no has comprendido el código del SPP. Te lo reescribo yo, pero revisa la ortografía porque voy muy rápido escribiendo.
La técnica \ac{SPP} consiste en organizar un histograma considerando la información espacial de los píxeles. En concreto, se basa en la concatenación de los histogramas, para niveles de subdivisión de la imagen. Si estamos en un nivel 1 por ejemplo, la imagen se divide en 4 secciones iguales, y se computan los histogramas por cada una de ellas, para luego ser concatenados para formar el vector final. Esto garantiza que el vector codifique de algún modo la información espacial de la imagen. A medida que aumentamos los niveles de la pirámide, aumentan de forma recurrente las divisiones y por lo tanto la granularidad del sistema. La técnica fue presentada originalmente para el problema del reconocimiento de escenas mediante visión por computador en el siguiente artículo \cite{spp_real}.
%TODO_DONE: Añade esta referencia: https://hal.inria.fr/inria-00548585/document y puedes meter como figura explicativa, una como la figura 1 de este paper.


Según el ejemplo de la figura \ref{fig:spp2}, podemos ver cómo se obtienen los histogramas para cada etiqueta de la imagen (representadas por círculos, diamantes y cruces). Para cada nivel de subdivisión obtenemos una serie de histogramas recogiendo los datos de las secciones de dichos niveles (como se puede ver en el nivel 1, por ejemplo). Finalmente, y tras recoger los histogramas generados se le aplica un peso a cada uno de ellos dependiendo del nivel que se use.

Una vez conseguíamos el descriptor generado en el paso anterior, éste se pasaba a diferentes sistemas de regresión (programados en MatLab): \textbf{Lineal}, \textbf{Lasso}, \textbf{Boosting Trees} y \ac{SVR}, de los cuales hablaremos más adelante. Para cada uno de ellos se añadía un cierto nivel de granularidad usando \ac{SPP} de hasta 3 niveles de agrupación para poder compararlos entre sí y saber cuál era mejor (\cite{isa2}). Para ello cogíamos los mejores resultados de cada sistema entre todos los niveles usados, y los usábamos como referencia del modelo $ISA^{2}$.

%TODO_DONE: toda esta subsección hay que moverla al capítulo de resultados, donde explicas la métrica

\section{Propuesta de modelo $ISA^{2}$ mejorado}
\label{sec:isa2_modelo_nuevo}
El trabajo fundamental de este TFG ha consistido en mejorar el sistema tradicional $ISA^{2}$. Nuestro objetivo con este TFG es el de conseguir un sistema $ISA^{2}$ que pueda funcionar en tiempo real, y que consuma pocos recursos de \ac{GPU}, para poder ser embebido en una plataforma de procesado dentro de un vehículo inteligente. La solución descrita en \cite{isa2} no cumplía ninguno de estos requisitos, principalmente por el sistema DeepLab para segmentación semántica que llevaba integrada. DeepLab es un modelo que da excelentes prestaciones en cuanto a segmentación semántica se refiere, pero no funciona en tiempo real, y además necesita una tarjeta de procesado con mucha memoria de \ac{GPU} y muy alto consumo.

Los modelos en tiempo real tienen, como principal objetivo, cumplir con los resultados de un modelo basado en \ac{DCNN} como DeepLab, ya que éstos son los que mejores prestaciones suelen dar debido a los recursos computacionales tan potentes que usan, sin los inconvenientes que ellos mismos traen consigo. Modelos como ERFNet (\cite{erfnet}) o ICNet (\cite{icnet}) lo consiguen usando arquitecturas más ligeras en comparación (aunque no sean las adecuadas para el reconocimiento de imágenes a gran escala).

Uno de los inconvenientes más importantes de los modelos en tiempo real es el campo de visión (\cite{swiftnet}). El campo de visión es realmente grande cuando se trabaja con modelos como DeepLab porque su arquitectura puede soportar tales cantidades de información mientras que en los modelos en tiempo real, éste se reduce. Es por ello que actualmente se tratan con técnicas como \textbf{pirámides de resolución} (\textit{resolution pyramids}) o \textbf{conexiones laterales} (\textit{lateral connections}) para compensarlo (\cite{res_pyr}, \cite{lat_con}).

Por otro lado, la principal ventaja (y la más evidente) de estos modelos es el bajo consumo de \ac{GPU} en comparación, además del uso de arquitecturas mucho más ligeras que en la otra clase de modelos. Esto hace posible que el procesado de imágenes sea más rápido y, por lo tanto, la toma de decisiones también.

Como ya hemos dicho lo ideal sería encontrar un modelo en tiempo real que pueda detectar los objetos de las imágenes con una precisión exquisita, y con un bajo consumo de la \ac{GPU}, y poco a poco se está consiguiendo.
%TODO_DONE: Sigue motivanto tú el trabajo, saca información del artículo siftwnet. en la intro se ve que hay familias de modelos que funcionan en real-time, ventajas, inconvenientes.


En resumen, para conseguir un modelo $ISA^{2}$ más rápido y eficaz, hemos decidido cambiar el sistema de segmentación semántica Deeplab por el conocido como \textbf{Swiftnet} (\cite{swiftnet}). La idea puede verse reflejada en la Figura \ref{fig:Isa_v2}.



Como se puede comprobar, el esquema de la figura \ref{fig:Isa_v2} sigue el mismo camino que el de la primera versión, salvo por unas modificaciones al principio que pasamos a explicar a continuación:

\begin{enumerate}

\item Las imágenes de entrada al modelo de \ac{SS} no llegan en diferentes escalas como se podría intuir. La razón de por qué es así es Swiftnet: el modelo admite tanto imágenes en diferentes escalas como imágenes con una única escala. Sin embargo, para este proyecto hemos optado por la recomendación de los autores (\cite{github_swiftnet}) y hemos decidido hacerlo con una única escala. De esta manera hemos obtenido los resultados esperados con la base de datos de \textbf{Cityscapes} (\cite{cityscapes}) que ellos mismos utilizaron (\cite{swiftnet}).

\item La segunda, y última, modificación es la más obvia: la sustitución del modelo de DeepLab por el modelo de Swiftnet (\cite{swiftnet}). A diferencia del primero, Swiftnet es un modelo que opera en tiempo real (\textbf{Real-Time}) de tal modo que cuando procesa una imagen lo hace en el momento, a una tasa de 39.9 \ac{FPS} (para la base de datos de Cityscapes, que es con la que extrajo este dato).%TODO_DONE: añade la velocidad a la que funciona en frames por segundo, seguro que viene en el paper original de siftnet.

\end{enumerate}


En el siguiente capítulo hablaremos acerca de la implementación de Swiftnet y de cómo es mejor para los campos de aplicación, especificados anteriormente, con respecto a DeepLab.
